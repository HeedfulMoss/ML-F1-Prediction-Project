{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Importing Data and Setup"],"metadata":{"id":"IhHoN8Ae9Aso"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from IPython.display import display\n","import os\n","import requests\n","from bs4 import BeautifulSoup\n","from dateutil.relativedelta import *"],"metadata":{"id":"sdWbm8pcgvAw","executionInfo":{"status":"ok","timestamp":1736193438670,"user_tz":300,"elapsed":113,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":145,"outputs":[]},{"cell_type":"code","source":["# URLs for the datasets\n","url_races = 'https://raw.githubusercontent.com/HeedfulMoss/ML-F1-Prediction-Project/9862ff3a11e86ee66101f55ad00515eceb177052/raw_datafiles/races.csv'\n","url_circuits = 'https://raw.githubusercontent.com/HeedfulMoss/ML-F1-Prediction-Project/9be04d41381da51770aaa26f1a73bc780806e5b3/raw_datafiles/circuits.csv'\n","url_drivers = 'https://raw.githubusercontent.com/HeedfulMoss/ML-F1-Prediction-Project/c74cb6be0e86191ba96d27be9aab2be024db1d31/raw_datafiles/drivers.csv'\n","url_results = 'https://raw.githubusercontent.com/HeedfulMoss/ML-F1-Prediction-Project/9be04d41381da51770aaa26f1a73bc780806e5b3/raw_datafiles/results.csv'\n","url_constructors = 'https://raw.githubusercontent.com/HeedfulMoss/ML-F1-Prediction-Project/9862ff3a11e86ee66101f55ad00515eceb177052/raw_datafiles/constructors.csv'\n","url_status = 'https://raw.githubusercontent.com/HeedfulMoss/ML-F1-Prediction-Project/9be04d41381da51770aaa26f1a73bc780806e5b3/raw_datafiles/status.csv'\n","url_driver_standings = 'https://raw.githubusercontent.com/HeedfulMoss/ML-F1-Prediction-Project/9be04d41381da51770aaa26f1a73bc780806e5b3/raw_datafiles/driver_standings.csv'\n","url_constructor_standings = 'https://raw.githubusercontent.com/HeedfulMoss/ML-F1-Prediction-Project/9be04d41381da51770aaa26f1a73bc780806e5b3/raw_datafiles/constructor_standings.csv'\n","url_qualifying = 'https://raw.githubusercontent.com/HeedfulMoss/ML-F1-Prediction-Project/9862ff3a11e86ee66101f55ad00515eceb177052/raw_datafiles/qualifying.csv'"],"metadata":{"id":"ceuiqMfFhBHa","executionInfo":{"status":"ok","timestamp":1736193438787,"user_tz":300,"elapsed":2,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":146,"outputs":[]},{"cell_type":"code","source":["races_df = pd.read_csv(url_races)\n","circuits_df = pd.read_csv(url_circuits)\n","drivers_df = pd.read_csv(url_drivers)\n","results_df = pd.read_csv(url_results)\n","constructors_df = pd.read_csv(url_constructors)\n","status_df = pd.read_csv(url_status)\n","driver_standings_df = pd.read_csv(url_driver_standings)\n","constructor_standings_df = pd.read_csv(url_constructor_standings)\n","qualifying_df = pd.read_csv(url_qualifying)"],"metadata":{"id":"SN9TZeMWhqCw","executionInfo":{"status":"ok","timestamp":1736193440333,"user_tz":300,"elapsed":1548,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":147,"outputs":[]},{"cell_type":"markdown","source":["# Formatted Races Data preprocessing"],"metadata":{"id":"Gf5PitHqbF0v"}},{"cell_type":"code","source":["# Keep only specified columns in races_df\n","races_df = races_df[['raceId', 'year', 'round', 'circuitId', 'name', 'date', 'time', 'url']]\n","\n","# Merge races_df with circuits_df based on circuitId\n","races_df = pd.merge(races_df, circuits_df[['circuitId', 'lat', 'lng', 'country', 'circuitRef']], on='circuitId', how='left')"],"metadata":{"id":"yigKUnjBj5Dz","executionInfo":{"status":"ok","timestamp":1736193440333,"user_tz":300,"elapsed":3,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":148,"outputs":[]},{"cell_type":"code","source":["formatted_races_df = races_df\n","\n","columns = ['year', 'round', 'circuitRef', 'lat', 'lng', 'country', 'date', 'url']\n","\n","# Reorder the columns in the DataFrame\n","formatted_races_df = races_df[columns]\n","\n","formatted_races_df.to_csv('formatted_races_df.csv', index=False)"],"metadata":{"id":"-wMWR5VeiPQQ","executionInfo":{"status":"ok","timestamp":1736193440333,"user_tz":300,"elapsed":2,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":149,"outputs":[]},{"cell_type":"markdown","source":["\n","# Formatted Results Data preprocessing"],"metadata":{"id":"QbXlsh4ymVeG"}},{"cell_type":"code","source":["# Merge the two dataframes based on the 'raceId' column.\n","results_df = pd.merge(results_df, races_df[['raceId', 'year', 'round', 'circuitRef', 'url']], on='raceId', how='left')\n","\n","# Merge results_df with drivers_df based on driverId\n","results_df = pd.merge(results_df, drivers_df[['driverId', 'driverRef', 'dob', 'nationality']], on='driverId', how='left')\n","\n","# Merge results_df with constructors_df based on constructorId\n","results_df = pd.merge(results_df, constructors_df[['constructorId', 'constructorRef']], on='constructorId', how='left')\n","\n","# Merge results_df with status_df based on statusId\n","results_df = pd.merge(results_df, status_df[['statusId', 'status']], on='statusId', how='left')"],"metadata":{"id":"z0Zctl6coMSv","executionInfo":{"status":"ok","timestamp":1736193440446,"user_tz":300,"elapsed":115,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":150,"outputs":[]},{"cell_type":"code","source":["# prompt: from results_df print the year 2019, from column order: year, round, circuitRef, driverRef, dob, nationality, constructorRef, grid, milliseconds, status, points, positionOrder, and url\n","formatted_results_df = results_df\n","\n","# Specify the desired column order\n","columns = ['year', 'round', 'circuitRef', 'driverRef', 'dob', 'nationality', 'constructorRef', 'grid', 'milliseconds', 'status', 'points', 'positionOrder','url']\n","\n","# Reorder the columns in the DataFrame\n","formatted_results_df = results_df[columns].copy()\n","\n","formatted_results_df['milliseconds'] = pd.to_numeric(formatted_results_df['milliseconds'], errors='coerce')\n","\n","\n","formatted_results_df.to_csv('formatted_results_df.csv', index=False)"],"metadata":{"id":"Hj5AZQoOtMLM","executionInfo":{"status":"ok","timestamp":1736193440862,"user_tz":300,"elapsed":418,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":151,"outputs":[]},{"cell_type":"markdown","source":["# Formatted Driver Standings Data preprocessing"],"metadata":{"id":"_cdB1-WD5Zcd"}},{"cell_type":"code","source":["# Define a function to compute the derived columns\n","def compute_previous_driver_values(df):\n","    df = df.sort_values(by=['year', 'round'])  # Sort by season and round\n","    df['post_race_driver_points'] = 0.0  # Initialize driver_points\n","    df['post_race_driver_wins'] = 0.0  # Initialize driver_wins\n","    df['post_race_driver_position'] = 0.0  # Initialize driver_standings_pos\n","\n","    # Group by season and driver to compute the values for previous rounds\n","    for (year, driver), group in df.groupby(['year', 'driverRef']):\n","        for idx in range(1, len(group)):\n","            prev_idx = group.index[idx - 1]\n","            current_idx = group.index[idx]\n","\n","            # Assign the values from the previous round\n","            df.at[current_idx, 'post_race_driver_points'] = df.at[prev_idx, 'points']\n","            df.at[current_idx, 'post_race_driver_wins'] = df.at[prev_idx, 'wins']\n","            df.at[current_idx, 'post_race_driver_position'] = df.at[prev_idx, 'position']\n","\n","    return df\n","\n","# Merge driver_standings_df with drivers_df based on driverId\n","driver_standings_df = pd.merge(driver_standings_df , drivers_df[['driverId','driverRef']], on='driverId', how='left')\n","\n","# Merge driver_standings_df with races_df based on raceId\n","driver_standings_df = pd.merge(driver_standings_df, races_df[['raceId','year','round']], on='raceId', how='left')\n","\n","# Apply the function to compute the derived columns\n","driver_standings_df = compute_previous_driver_values(driver_standings_df)"],"metadata":{"id":"15-_TNc5VmIU","executionInfo":{"status":"ok","timestamp":1736193446835,"user_tz":300,"elapsed":5975,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":152,"outputs":[]},{"cell_type":"code","source":["# Specify the desired column order\n","formatted_driver_standings_df = driver_standings_df\n","\n","columns = ['year', 'round', 'driverRef', 'points','wins', 'position', 'post_race_driver_points', 'post_race_driver_wins', 'post_race_driver_position']\n","\n","# Reorder the columns in the DataFrame\n","formatted_driver_standings_df = formatted_driver_standings_df[columns]\n","\n","# prompt: rename wins to driver_wins column from driver_standings\n","formatted_driver_standings_df = formatted_driver_standings_df.rename(columns={'points': 'driver_points'})\n","formatted_driver_standings_df = formatted_driver_standings_df.rename(columns={'wins': 'driver_wins'})\n","formatted_driver_standings_df = formatted_driver_standings_df.rename(columns={'position': 'driver_position'})\n","\n","formatted_driver_standings_df.drop(['driver_points', 'driver_wins', 'driver_position'],axis = 1, inplace = True)\n","\n","formatted_driver_standings_df.to_csv('formatted_driver_standings_df.csv', index=False)"],"metadata":{"id":"DUitaUAJzCKa","executionInfo":{"status":"ok","timestamp":1736193446958,"user_tz":300,"elapsed":125,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":153,"outputs":[]},{"cell_type":"markdown","source":["# Formatted Constructor Standings Data preprocessing"],"metadata":{"id":"RvjRK4JB8nNb"}},{"cell_type":"code","source":["# Define a function to compute the derived columns\n","def compute_previous_constructor_values(df):\n","    df = df.sort_values(by=['year', 'round'])  # Sort by season and round\n","    df['post_race_constructor_points'] = 0.0  # Initialize driver_points\n","    df['post_race_constructor_wins'] = 0.0  # Initialize driver_wins\n","    df['post_race_constructor_position'] = 0.0  # Initialize driver_standings_pos\n","\n","    # Group by season and driver to compute the values for previous rounds\n","    for (year, driver), group in df.groupby(['year', 'constructorRef']):\n","        for idx in range(1, len(group)):\n","            prev_idx = group.index[idx - 1]\n","            current_idx = group.index[idx]\n","\n","            # Assign the values from the previous round\n","            df.at[current_idx, 'post_race_constructor_points'] = df.at[prev_idx, 'points']\n","            df.at[current_idx, 'post_race_constructor_wins'] = df.at[prev_idx, 'wins']\n","            df.at[current_idx, 'post_race_constructor_position'] = df.at[prev_idx, 'position']\n","\n","    return df\n","\n","# Merge constructor_standings_df with constructors_df based on constructorId\n","constructor_standings_df = pd.merge(constructor_standings_df , constructors_df[['constructorId','constructorRef']], on='constructorId', how='left')\n","\n","# Merge constructor_standings_df with races_df based on raceId\n","constructor_standings_df = pd.merge(constructor_standings_df , races_df[['raceId','year','round']], on='raceId', how='left')\n","\n","# Apply the function to compute the derived columns\n","constructor_standings_df = compute_previous_constructor_values(constructor_standings_df)"],"metadata":{"id":"kVtkSMztcCmy","executionInfo":{"status":"ok","timestamp":1736193448926,"user_tz":300,"elapsed":1969,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":154,"outputs":[]},{"cell_type":"code","source":["# Specify the desired column order\n","formatted_constructor_standings_df = constructor_standings_df\n","\n","columns = ['year', 'round', 'constructorRef', 'points','wins', 'position', 'post_race_constructor_points', 'post_race_constructor_wins', 'post_race_constructor_position']\n","\n","# Reorder the columns in the DataFrame\n","formatted_constructor_standings_df = formatted_constructor_standings_df[columns]\n","\n","# prompt: rename wins to driver_wins column from driver_standings\n","formatted_constructor_standings_df = formatted_constructor_standings_df.rename(columns={'points': 'constructor_points'})\n","formatted_constructor_standings_df = formatted_constructor_standings_df.rename(columns={'wins': 'constructor_wins'})\n","formatted_constructor_standings_df = formatted_constructor_standings_df.rename(columns={'position': 'constructor_position'})\n","\n","formatted_constructor_standings_df.drop(['constructor_points', 'constructor_wins', 'constructor_position'],axis = 1, inplace = True)\n","\n","formatted_constructor_standings_df.to_csv('formatted_constructor_standings_df.csv', index=False)"],"metadata":{"id":"IvpyEpgWdJ-f","executionInfo":{"status":"ok","timestamp":1736193448926,"user_tz":300,"elapsed":2,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":155,"outputs":[]},{"cell_type":"markdown","source":["# Formatted Qualifying Data preprocessing"],"metadata":{"id":"Hk6wBiymG8mC"}},{"cell_type":"code","source":["# Merge qualifying.csv with constructors.csv to add the constructorRef column\n","qualifying_df  = qualifying_df.merge(constructors_df[['constructorId', 'name']], on='constructorId', how='left')\n","\n","# Merge the result with races.csv to add the year column\n","qualifying_df  = qualifying_df.merge(races_df[['raceId', 'year']], on='raceId', how='left')\n","\n","# Merge the result with races.csv to add the year column\n","qualifying_df  = qualifying_df.merge(races_df[['raceId', 'round']], on='raceId', how='left')\n","\n","# Merge the result with races.csv to add the year column\n","qualifying_df  = qualifying_df.merge(drivers_df[['driverId','forename','surname']], on='driverId', how='left')\n","\n","# Create the 'fullname' column by combining 'forename' and 'surname'\n","qualifying_df['fullname'] = qualifying_df['forename'] + ' ' + qualifying_df['surname']\n","\n","qualifying_df[['q1', 'q2', 'q3']] = qualifying_df[['q1', 'q2', 'q3']].replace(r'\\\\N', np.nan, regex=True)\n","\n","qualifying_df['q'] = qualifying_df['q3'].fillna(qualifying_df['q2']).fillna(qualifying_df['q1'])\n"],"metadata":{"id":"utb_LGTjmkS0","executionInfo":{"status":"ok","timestamp":1736193449071,"user_tz":300,"elapsed":147,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":156,"outputs":[]},{"cell_type":"code","source":["formatted_qualifying_df = qualifying_df\n","\n","formatted_qualifying_df = formatted_qualifying_df.rename(columns={'position': 'qualifying_position'})\n","\n","columns = ['qualifying_position', 'fullname', 'name', 'q', 'year', 'round']\n","\n","# Reorder the columns in the DataFrame\n","formatted_qualifying_df = formatted_qualifying_df[columns]\n","\n","#print(updated_qualifying[['q']].isnull().sum())\n","formatted_qualifying_df = formatted_qualifying_df.dropna(subset=['q'])\n","\n","formatted_qualifying_df.rename(columns = {'qualifying_position': 'grid'}, inplace = True)\n","\n","# Save the updated qualifying DataFrame to a new file\n","formatted_qualifying_df.to_csv('formatted_qualifying_df.csv' , index=False)"],"metadata":{"id":"YuegHaKfnnDH","executionInfo":{"status":"ok","timestamp":1736193449071,"user_tz":300,"elapsed":3,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":157,"outputs":[]},{"cell_type":"markdown","source":["# Formatted Weather Data preprocessing"],"metadata":{"id":"iC5d6rr0HCHE"}},{"cell_type":"code","source":["# Dictionary of weather conditions\n","weather_dict = {\n","    'weather_warm': ['soleggiato', 'clear', 'warm', 'hot', 'sunny', 'fine', 'mild', 'sereno', 'ensoleillé', 'chaud', 'doux', 'beau','heiß', 'sonnig', 'mild'],\n","    'weather_cold': ['cold', 'fresh', 'chilly', 'cool', 'froid', 'frais', 'glacial','kalt'],\n","    'weather_dry': ['dry', 'asciutto', 'sec','trocken'],\n","    'weather_wet': ['showers', 'wet', 'rain', 'pioggia', 'damp', 'thunderstorms', 'rainy', 'pluie', 'humide', 'averses','regen', 'nass', 'feucht', 'gewitter', 'schauer'],\n","    'weather_cloudy': ['overcast', 'nuvoloso', 'clouds', 'cloudy', 'grey', 'coperto', 'nuageux', 'gris','bewölkt', 'wolkig', 'trüb']\n","}\n","\n","def analyze_weather(input_string):\n","    found_conditions = []\n","    input_string = input_string.lower()  # Convert input to lowercase for case-insensitive matching\n","    for condition, keywords in weather_dict.items():\n","        for keyword in keywords:\n","            if keyword in input_string:\n","                found_conditions.append(condition)\n","                break  # Move to the next condition once a match is found\n","    return found_conditions\n","\n","def get_race_weather(url):\n","    try:\n","        # Fetch the page content\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise an exception if there's an error in the request\n","        # Parse the page with BeautifulSoup\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        # Wikipedia infobox tables have class 'infobox'\n","        infobox = soup.find('table', class_='infobox')\n","        if not infobox:\n","            return None\n","\n","        # Find all rows in the infobox\n","        rows = infobox.find_all('tr')\n","\n","        # Iterate through each row to find the \"Weather\" row\n","        for row in rows:\n","            header = row.find('th')\n","            if header and \"Weather\" in header.get_text():\n","                data_cell = row.find('td')\n","                if data_cell:\n","                    return data_cell.get_text(strip=True)\n","        return None\n","    except requests.RequestException as e:\n","        print(f\"Error fetching URL {url}: {e}\")\n","        return None\n","\n","def get_italian_weather(url):\n","    try:\n","        # Use the Wikipedia API to find the Italian equivalent of the page\n","        api_url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=langlinks&format=json&lllang=it&titles={url.split('/')[-1]}\"\n","        response = requests.get(api_url)\n","        response.raise_for_status()\n","        data = response.json()\n","\n","        # Extract the Italian page title\n","        pages = data.get('query', {}).get('pages', {})\n","        for page_id, page_info in pages.items():\n","            if 'langlinks' in page_info:\n","                italian_title = page_info['langlinks'][0]['*']\n","                italian_url = f\"https://it.wikipedia.org/wiki/{italian_title.replace(' ', '_')}\"\n","                print(f\"Translated Italian URL: {italian_url}\")\n","\n","                # Fetch and parse the Italian page\n","                response = requests.get(italian_url)\n","                response.raise_for_status()\n","                soup = BeautifulSoup(response.text, 'html.parser')\n","\n","                # Search for \"Clima\" instead of \"Weather\"\n","                infobox = soup.find('table', class_='infobox')\n","                if not infobox:\n","                    return None\n","\n","                rows = infobox.find_all('tr')\n","                for row in rows:\n","                    header = row.find('th')\n","                    if header and \"Clima\" in header.get_text():\n","                        data_cell = row.find('td')\n","                        if data_cell:\n","                            return data_cell.get_text(strip=True)\n","                print(f\"No 'Clima' section found on Italian page: {italian_url}\")\n","                return None\n","\n","        # Fall back if no translation is found\n","        print(f\"No Italian translation found via API for URL: {url}\")\n","        return None\n","\n","    except requests.RequestException as e:\n","        print(f\"Error fetching Italian URL {url}: {e}\")\n","        return None\n","\n","def get_french_weather(url):\n","    \"\"\"\n","    Attempts to find French Wikipedia page via API, then parses\n","    any table row containing 'Météo' or 'Climat' in the <th> cell\n","    and returns the <td> text.\n","    \"\"\"\n","\n","    try:\n","        # 1) Use the Wikipedia API to find the French equivalent of the page\n","        api_url = (\n","            \"https://en.wikipedia.org/w/api.php?\"\n","            f\"action=query&prop=langlinks&format=json&lllang=fr&titles={url.split('/')[-1]}\"\n","        )\n","        response = requests.get(api_url)\n","        response.raise_for_status()\n","        data = response.json()\n","\n","        # 2) Extract the French page title\n","        pages = data.get(\"query\", {}).get(\"pages\", {})\n","        for page_id, page_info in pages.items():\n","            if \"langlinks\" in page_info:\n","                french_title = page_info[\"langlinks\"][0][\"*\"]\n","                french_url = f\"https://fr.wikipedia.org/wiki/{french_title.replace(' ', '_')}\"\n","                print(f\"Translated French URL: {french_url}\")\n","\n","                # 3) Fetch and parse the French page\n","                fr_response = requests.get(french_url)\n","                fr_response.raise_for_status()\n","                soup = BeautifulSoup(fr_response.text, \"html.parser\")\n","\n","                # 4) Search across all tables, since not all French GP pages use 'infobox' class\n","                tables = soup.find_all(\"table\")\n","                for table in tables:\n","                    rows = table.find_all(\"tr\")\n","                    for row in rows:\n","                        header = row.find(\"th\")\n","                        if header:\n","                            header_text = header.get_text(strip=True)\n","                            # Check if \"Météo\" or \"Climat\" is in the <th> text\n","                            if \"Météo\" in header_text or \"Climat\" in header_text:\n","                                data_cell = row.find(\"td\")\n","                                if data_cell:\n","                                    return data_cell.get_text(strip=True)\n","\n","                print(f\"No 'Météo' or 'Climat' section found on French page: {french_url}\")\n","                return None\n","\n","        # 5) Fall back if no translation is found\n","        print(f\"No French translation found via API for URL: {url}\")\n","        return None\n","\n","    except requests.RequestException as e:\n","        print(f\"Error fetching French URL {url}: {e}\")\n","        return None\n","\n","def get_german_weather(url):\n","    \"\"\"\n","    Attempts to find the German Wikipedia page via API, then parses\n","    tables to locate a row whose first cell contains 'Wetter'.\n","    Returns the text in the second cell of that row.\n","    \"\"\"\n","    try:\n","        # Step 1) Use the Wikipedia API to find the German equivalent of the page\n","        api_url = (\n","            \"https://en.wikipedia.org/w/api.php?\"\n","            f\"action=query&prop=langlinks&format=json&lllang=de&titles={url.split('/')[-1]}\"\n","        )\n","        response = requests.get(api_url)\n","        response.raise_for_status()\n","        data = response.json()\n","\n","        # Step 2) Extract the German page title\n","        pages = data.get(\"query\", {}).get(\"pages\", {})\n","        for page_id, page_info in pages.items():\n","            if \"langlinks\" in page_info:\n","                german_title = page_info[\"langlinks\"][0][\"*\"]\n","                german_url = f\"https://de.wikipedia.org/wiki/{german_title.replace(' ', '_')}\"\n","                print(f\"Translated German URL: {german_url}\")\n","\n","                # Step 3) Fetch and parse the German page\n","                de_response = requests.get(german_url)\n","                de_response.raise_for_status()\n","                soup = BeautifulSoup(de_response.text, \"html.parser\")\n","\n","                # Step 4) Search across all tables for any row whose first cell contains 'Wetter'\n","                tables = soup.find_all(\"table\")\n","                for table in tables:\n","                    rows = table.find_all(\"tr\")\n","                    for row in rows:\n","                        # Grab all cells in the row (both <th> and <td>)\n","                        cells = row.find_all([\"th\", \"td\"])\n","                        if len(cells) >= 2:\n","                            # Convert the first cell's text to lowercase and check if 'wetter' appears\n","                            first_cell_text = cells[0].get_text(strip=True).lower()\n","                            # If we see \"wetter\" (with or without the colon) in this cell\n","                            if \"wetter\" in first_cell_text:\n","                                # The second cell presumably contains the weather description\n","                                return cells[1].get_text(strip=True)\n","\n","                print(f\"No 'Wetter' section found on German page: {german_url}\")\n","                return None\n","\n","        # Step 5) Fall back if no German translation is found via the API\n","        print(f\"No German translation found via API for URL: {url}\")\n","        return None\n","\n","    except requests.RequestException as e:\n","        print(f\"Error fetching German URL {url}: {e}\")\n","        return None\n","\n","\n","# -------------------\n","# Main script logic\n","# -------------------\n","\n","# 1) Check if updated_weather_data.csv exists\n","csv_filename = 'updated_weather_data.csv'\n","if not os.path.exists(csv_filename):\n","    # If the file DOES NOT exist, use formatted_races_df to create weather_df\n","    weather_df = formatted_races_df.copy()\n","\n","    # Initialize columns for each weather category with 0\n","    for col in weather_dict.keys():\n","        weather_df[col] = 0\n","\n","    # Add a column to store raw weather information\n","    weather_df['weather'] = ''\n","\n","else:\n","    # If it DOES exist, read it into weather_df\n","    weather_df = pd.read_csv(csv_filename)\n","\n","# 2) Process each row in the DataFrame.\n","#    If we're continuing from a partial file, only fill rows where 'weather' is blank.\n","rows_to_process = weather_df[weather_df['weather'].isna() | weather_df['weather'].eq('')]\n","\n","for index, row in rows_to_process.iterrows():\n","    print(f\"Processing year: {row['year']}, round: {row['round']}\")\n","\n","    weather = get_race_weather(row['url'])\n","    if not weather:\n","        print(f\"Weather information not found for URL: {row['url']}. Trying Italian version.\")\n","        weather = get_italian_weather(row['url'])\n","\n","    if not weather:\n","        print(f\"Weather information not found for Italian URL. Trying French version.\")\n","        weather = get_french_weather(row['url'])\n","\n","    if not weather:\n","        print(f\"Weather not found for French URL. Trying German version.\")\n","        weather = get_german_weather(row['url'])\n","\n","    if weather:\n","        print(f\"Weather: {weather}\")\n","        weather_df.at[index, 'weather'] = weather  # Store raw weather information\n","        weather_conditions = analyze_weather(weather)\n","        print(\"Detected weather conditions:\", weather_conditions)\n","        print(\"\")\n","        for condition in weather_conditions:\n","            weather_df.at[index, condition] = 1\n","    else:\n","        print(f\"Weather information not found for all language versions of URL: {row['url']}\")\n","        print(\"Weather data needs manual review.\")\n","\n","# 3) Save the updated DataFrame to a CSV file\n","weather_df.to_csv(csv_filename, index=False)\n","print(f\"Data saved to {csv_filename}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XhfYcGzX5FUX","outputId":"fbcc2c6d-5ee9-479e-a78d-0e78d0204e7c","executionInfo":{"status":"ok","timestamp":1736193449220,"user_tz":300,"elapsed":151,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":158,"outputs":[{"output_type":"stream","name":"stdout","text":["Data saved to updated_weather_data.csv\n"]}]},{"cell_type":"code","source":["formatted_weather_df = weather_df\n","\n","columns = ['year', 'round', 'circuitRef','weather','weather_warm','weather_cold','weather_dry','weather_wet','weather_cloudy']\n","\n","# Reorder the columns in the DataFrame\n","formatted_weather_df = formatted_weather_df[columns]\n","\n","formatted_weather_df.to_csv('formatted_weather_df', index=False)"],"metadata":{"id":"5Qv1d5auweZK","executionInfo":{"status":"ok","timestamp":1736193449220,"user_tz":300,"elapsed":2,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":159,"outputs":[]},{"cell_type":"markdown","source":["# Final Dataframe preprocessing"],"metadata":{"id":"LaHJnRMoIfOa"}},{"cell_type":"code","source":["final_df = (\n","    formatted_races_df\n","    .merge(formatted_weather_df, on=['year', 'round', 'circuitRef'], how='inner')\n","    .drop(['lat', 'lng', 'country', 'weather'], axis=1)\n","    .merge(formatted_results_df, on=['year', 'round', 'circuitRef', 'url'], how='inner')\n","    .drop(['url', 'points', 'status', 'milliseconds'], axis=1)\n","    .merge(formatted_driver_standings_df, on=['year', 'round', 'driverRef'], how='left')\n","    .merge(formatted_constructor_standings_df, on=['year', 'round', 'constructorRef'], how='left')\n","    .merge(formatted_qualifying_df, on=['year', 'round', 'grid'], how='inner')\n","    .drop(['fullname', 'name'], axis=1)\n",")"],"metadata":{"id":"--zH3faIwa4O","executionInfo":{"status":"ok","timestamp":1736193449220,"user_tz":300,"elapsed":1,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":160,"outputs":[]},{"cell_type":"code","source":["# calculate age of drivers\n","final_df['date'] = pd.to_datetime(final_df.date)\n","final_df['dob'] = pd.to_datetime(final_df.dob)\n","final_df['driver_age'] = final_df.apply(lambda x: relativedelta(x['date'], x['dob']).years, axis=1)\n","final_df.drop(['date', 'dob'], axis = 1, inplace = True)"],"metadata":{"id":"dcedqoPNCPYK","executionInfo":{"status":"ok","timestamp":1736193449986,"user_tz":300,"elapsed":767,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":161,"outputs":[]},{"cell_type":"code","source":["# Fill/drop nulls\n","for col in ['post_race_driver_points', 'post_race_driver_wins', 'post_race_driver_position',\n","            'post_race_constructor_points', 'post_race_constructor_wins', 'post_race_constructor_position']:\n","    final_df[col] = final_df[col].fillna(0).astype(int)\n","\n","# Drop remaining nulls\n","final_df.dropna(inplace=True)"],"metadata":{"id":"oe45g9x2C659","executionInfo":{"status":"ok","timestamp":1736193449986,"user_tz":300,"elapsed":3,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":162,"outputs":[]},{"cell_type":"code","source":["# convert to boolean\n","for col in ['weather_warm', 'weather_cold','weather_dry', 'weather_wet', 'weather_cloudy']:\n","    final_df[col] = final_df[col].map(lambda x: bool(x))"],"metadata":{"id":"1Na3mjECdxm9","executionInfo":{"status":"ok","timestamp":1736193449986,"user_tz":300,"elapsed":2,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":163,"outputs":[]},{"cell_type":"code","source":["# calculate difference in qualifying times\n","final_df['q'] = final_df.q.map(lambda x: 0 if str(x) == '00.000'\n","                             else(float(str(x).split(':')[1]) + (60 * float(str(x).split(':')[0])) if x != 0 else 0))\n","final_df = final_df[final_df['q'] != 0]\n","final_df.sort_values(['year', 'round', 'grid'], inplace = True)\n","final_df['qualifying_time_diff'] = final_df.groupby(['year', 'round']).q.diff()\n","final_df['q'] = final_df.groupby(['year', 'round']).qualifying_time_diff.cumsum().fillna(0)\n","final_df.drop('qualifying_time_diff', axis = 1, inplace = True)"],"metadata":{"id":"xCoO07bAd87l","executionInfo":{"status":"ok","timestamp":1736193450108,"user_tz":300,"elapsed":124,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":164,"outputs":[]},{"cell_type":"code","source":["# Convert categorical columns to dummy variables in final_df\n","final_df = pd.get_dummies(\n","    final_df,\n","    columns=['circuitRef', 'nationality', 'constructorRef'],\n","    dtype=int\n",")\n","\n","# Drop columns based on sum thresholds\n","for col in final_df.columns:\n","    if 'nationality' in col and final_df[col].sum() < 140:\n","        final_df.drop(col, axis=1, inplace=True)\n","    elif 'constructorRef' in col and final_df[col].sum() < 140:\n","        final_df.drop(col, axis=1, inplace=True)\n","    elif 'circuitRef' in col and final_df[col].sum() < 70:\n","        final_df.drop(col, axis=1, inplace=True)\n","    else:\n","        pass"],"metadata":{"id":"Bm50oXWheYcO","executionInfo":{"status":"ok","timestamp":1736193450336,"user_tz":300,"elapsed":229,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":165,"outputs":[]},{"cell_type":"code","source":["final_df.to_csv('final_df.csv', index = False)"],"metadata":{"id":"pJaOiFXrfWBz","executionInfo":{"status":"ok","timestamp":1736187299290,"user_tz":300,"elapsed":291,"user":{"displayName":"Alex Ayerbe","userId":"04944963069189692272"}}},"execution_count":118,"outputs":[]}]}